import os
import shutil
import json
import subprocess

# =============================
# ä½¿ç”¨è€…åƒæ•¸
# =============================
dataset_id = 1   # nnU-Net çš„ dataset number
dataset_name = f"Dataset{dataset_id:03d}_CT"
nnunet_raw = "nnUNet_raw"   # nnU-Net raw è³‡æ–™å¤¾
gpu_id = "0"                # è¨“ç·´è¦ç”¨çš„ GPU ç·¨è™Ÿ
output_dir = "./predictions" # æ¨è«–è¼¸å‡ºè³‡æ–™å¤¾

# åŸå§‹è³‡æ–™å¤¾
train_img_dir = "training_image"
train_lbl_dir = "training_label"
test_img_dir = "testing_image"

# =============================
# å»ºç«‹è³‡æ–™å¤¾
# =============================
dataset_dir = os.path.join(nnunet_raw, dataset_name)
imagesTr = os.path.join(dataset_dir, "imagesTr")
labelsTr = os.path.join(dataset_dir, "labelsTr")
imagesTs = os.path.join(dataset_dir, "imagesTs")

os.makedirs(imagesTr, exist_ok=True)
os.makedirs(labelsTr, exist_ok=True)
os.makedirs(imagesTs, exist_ok=True)

# =============================
# è¤‡è£½è¨“ç·´å½±åƒ & æ¨™è¨»
# =============================
train_cases = sorted([f for f in os.listdir(train_img_dir) if f.endswith(".nii.gz")])
for case in train_cases:
    case_id = case.replace(".nii.gz", "")
    shutil.copy(os.path.join(train_img_dir, case), os.path.join(imagesTr, f"{case_id}_0000.nii.gz"))
    shutil.copy(os.path.join(train_lbl_dir, case), os.path.join(labelsTr, f"{case_id}.nii.gz"))

# =============================
# è¤‡è£½æ¸¬è©¦å½±åƒ
# =============================
test_cases = sorted([f for f in os.listdir(test_img_dir) if f.endswith(".nii.gz")])
for case in test_cases:
    case_id = case.replace(".nii.gz", "")
    shutil.copy(os.path.join(test_img_dir, case), os.path.join(imagesTs, f"{case_id}_0000.nii.gz"))

# =============================
# å»ºç«‹ dataset.json
# =============================
dataset_json = {
    "name": "CT_Segmentation",
    "description": "CT organ segmentation",
    "tensorImageSize": "3D",
    "modality": {"0": "CT"},
    "labels": {
        "0": "background",
        "1": "organ"
    },
    "numTraining": len(train_cases),
    "numTest": len(test_cases),
    "training": [
        {
            "image": f"./imagesTr/{case.replace('.nii.gz', '')}_0000.nii.gz",
            "label": f"./labelsTr/{case.replace('.nii.gz','')}.nii.gz"
        } for case in train_cases
    ],
    "test": [
        f"./imagesTs/{case.replace('.nii.gz','')}_0000.nii.gz" for case in test_cases
    ]
}

with open(os.path.join(dataset_dir, "dataset.json"), "w") as f:
    json.dump(dataset_json, f, indent=4)

print("âœ… å·²å®Œæˆ nnU-Net v2 è³‡æ–™æº–å‚™ï¼")
print(f"è³‡æ–™é›†å­˜æ”¾æ–¼: {dataset_dir}")

# =============================
# åŸ·è¡Œ nnU-Net pipeline
# =============================

print("\nğŸš€ é–‹å§‹è³‡æ–™å‰è™•ç†...")
subprocess.run(["nnUNetv2_plan_and_preprocess", "-d", str(dataset_id), "--verify_dataset_integrity"])

print("\nğŸš€ é–‹å§‹è¨“ç·´ (3d_fullres, fold=0)...")
subprocess.run(["nnUNetv2_train", str(dataset_id), "3d_fullres", "0", "-g", gpu_id])

print("\nğŸš€ é–‹å§‹æ¨è«–...")
os.makedirs(output_dir, exist_ok=True)
subprocess.run([
    "nnUNetv2_predict",
    "-d", str(dataset_id),
    "-i", imagesTs,
    "-o", output_dir,
    "-f", "all",
    "-g", gpu_id
])

print("\nğŸ‰ å…¨æµç¨‹å®Œæˆï¼æ¨è«–çµæœå·²å­˜æ”¾æ–¼:", output_dir)
